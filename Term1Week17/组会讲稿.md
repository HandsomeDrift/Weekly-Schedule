# 组会讲稿

## CPN1

我们用 $Y_p \in Z \subseteq \mathbb{R}^2$ 表示 $P$ 个解剖学标志点（即部位）的像素位置，其中 $Z$ 是图像中的所有 $(u, v)$ 位置的集合。我们的目标是预测所有 $P$ 个部位的位置 $Y = (Y_1, \dots, Y_P)$。

一个姿态机器【29】（见图 2(a) 和 2(b)）由一系列==多类别预测器 $g_t(\cdot)$== 组成，这些预测器被训练用于在层级结构的每一级预测每个部位的位置。在每个阶段 $t \in {1, \dots, T}$ 中，==分类器 $g_t$==用于预测每个部位的信念值，从而为每个部位 $Y_p = z, \forall z \in Z$ 生成belief map：
$$
g_1(x_z) \to \{b_{p1}(Y_p = z)\}_{p \in \{0, \dots, P\}},
$$
其中，==$b_{p1}(Y_p = z)$ 是分类器 $g_1$ 在图像位置 $z$ 为第 $p$ 个部位预测的得分==。我们将所有部位 $p$ 在图像每个位置 $z = (u, v)^T$ 处的信念表示为 $b_{p1} \in \mathbb{R}^{w \times h}$，其中 $w$ 和 $h$ 分别是图像的宽度和高度，即：
$$
b_{pt}[u, v] = b_{pt}(Y_p = z)。
$$
为简化表示，我们将所有部位的belief map集合表示为 $b_t \in \mathbb{R}^{w \times h \times (P+1)}$（==$P$ 个部位加一个背景==）。

在后续阶段中，分类器预测每个部位的信念 $Y_p = z, \forall z \in Z$，此时的输入包括：(1) 图像数据 $x_t^z \in \mathbb{R}^d$ 的特征，以及 (2) 来自上一阶段的分类器的belief map的上下文信息：
$$
g_t(x'_z, \psi_t(z, b_{t-1})) \to \{b_{pt}(Y_p = z)\}_{p \in \{0, \dots, P+1\}}，
$$
其中，$\psi_{t>1}(\cdot)$ 是从 $b_{t-1}$ 到上下文特征的映射。在每个阶段中，计算的belief map会逐步优化每个部位的位置估计。需要注意的是，==后续阶段的图像特征 $x'_z$ 允许不同于第一阶段的图像特征 $x$==。文献【29】中提出的姿态机器使用提升随机森林作为预测器（$g_t$），所有阶段中使用固定的手工设计图像特征（$x' = x$），以及固定的手工设计==上下文特征映射（$\psi_t(\cdot)$）==来捕捉空间上下文。

## CPN2

==第一阶段的belief map是由一个小感受野的网络从局部图像信息中生成的。在第二阶段中，我们设计了一个网络，通过极大地增加有效感受野以捕获长距离的空间交互==。==大感受野==可以通过以下方式实现：

1. **池化操作**，但可能会以精度为代价；
2. **增大卷积核大小**，但会增加模型的参数数量；
3. **增加卷积层的数量**，但可能会导致训练过程中出现梯度消失问题。

我们在图 2(d) 中展示了第二阶段及后续阶段（$t \geq 2$）的网络设计和对应的感受野。在belief map上，我们==使用多层卷积来实现大的感受野，而不是依赖步幅较大的池化操作==。这种设计能够==以较少的模型参数实现大的感受野，同时保持较高的精度==。实验表明，即使在高精度范围内，步幅为 8 的网络与步幅为 4 的网络表现相当，但步幅为 8 的设计更容易实现较大的感受野。

## CPN3

我们通过在网络的每个阶段输出处定义一个损失函数来鼓励网络不断生成这样的表示，==该损失函数最小化预测belief map与理想belief map之间的 $l_2$ 距离==。==对于某个部位 $p$ 的理想belief map记为 $b_p^*(Y_p = z)$，其通过在每个身体部位 $p$ 的真实位置放置高斯峰值生成==。我们在每个阶段 $t$ 的输出处希望最小化的代价函数为：
$$
f_t = \sum_{p=1}^{P+1} \sum_{z \in Z} \| b_p^t(z) - b_p^*(z) \|_2^2。
$$
==完整架构的总体目标是将每个阶段的损失相加==，其表示为：
$$
F = \sum_{t=1}^T f_t。
$$
我们使用标准的随机梯度下降（Stochastic Gradient Descent, SGD）对网络的所有 $T$ 个阶段进行联合训练。为了在所有后续阶段共享图像特征 $x'$，我们在 $t \geq 2$ 的阶段中共享对应卷积层的权重（见图2）。

## Hourglass

每个Hourglass module的结构如下图所示，其中每个box都是一个残差结构。可以看到在top-down阶段，对于两个**相邻**分辨率的feature map，我们通过upsampling（这里用的是nearest neighbor upsampling）对较低分辨率的feature map进行上采样，然后通过**skip connection**将bottom-up阶段较高分辨率的feature map拿过来，此时再通过element-wise addition将这两部分特征进行合并。通过这种方式，在top-down阶段将不同分辨率下的特征逐步进行了融合。

## CPN1

我们的网络架构包括两个阶段：GlobalNet 和 RefineNet。==GlobalNet基于特征金字塔网络[24]学习良好的特征表示==。更重要的是，金字塔特征表示能够提供足够的上下文信息，这是推理遮挡和不可见关节所必需的。基于金字塔特征，==RefineNet通过在线难关键点挖掘损失显式地处理这些“难”关键点==。

## CPN2

### **GlobalNet 的结构**

1. **基于 ResNet 的网络架构**：
   - **GlobalNet** 通常基于成熟的卷积神经网络架构，如 **ResNet**，作为骨干网络。ResNet 的不同卷积层会生成不同尺度的特征图，例如 C2,C3,C4,C5C_2, C_3, C_4, C_5C2,C3,C4,C5 层，每一层的特征图提供了不同层次的空间分辨率和语义信息。
   - 浅层特征（如 C2C_2C2 和 C3C_3C3）包含较多的细节信息，但语义信息较少；而深层特征（如 C4C_4C4 和 C5C_5C5）包含丰富的语义信息，但空间分辨率较低。
2. **特征金字塔结构**：
   - 在 **GlobalNet** 中，采用了一种类似于 **特征金字塔网络（FPN）** 的结构，逐步提取不同尺度的特征。通过上采样和下采样操作，**GlobalNet** 融合了不同层次的特征图，增强了对图像中目标的全局上下文理解。
   - 在 **GlobalNet** 中，上采样和下采样层的融合有助于同时保持较高的空间分辨率和丰富的语义信息。
3. **多尺度特征融合**：
   - **GlobalNet** 通过融合不同尺度的特征图来处理多尺度目标，确保网络能够同时识别大目标和小目标，尤其是在复杂场景中的小关节点（如手指、脚趾等）的定位。
   - 在网络的不同层次，浅层特征负责提供细节信息，深层特征负责提供全局语义信息。通过融合这些信息，GlobalNet 能够提高关节定位的准确性。
4. **特征整合与逐元素相加**：
   - 在多尺度特征融合过程中，**GlobalNet** 在不同特征图之间进行逐元素相加（element-wise addition），确保每个层的特征信息都能够有效传递到后续层。
   - 为了确保通道数匹配，通常使用 **$1 \times 1$ 卷积** 来调整不同层特征图的通道数，从而使得它们能够顺利进行相加操作。

## CPN3

### **RefineNet 的工作流程**

假设输入图像的大小为 $512 \times 512$：

1. **特征提取**：
   - 使用 ResNet 提取不同层次的特征（例如 $C2$, $C3$, $C4$, $C5$）。
   - 特征分辨率从高到低，语义信息从弱到强。
2. **特征细化**：
   - 将深层特征（低分辨率，强语义）通过上采样恢复到更高分辨率。
   - 与对应浅层特征进行融合，结合语义和细节信息。
3. **逐步细化**：
   - 多层 RefineNet 模块串联，逐层细化特征。
   - 最终生成高分辨率的分割预测。
4. **分割结果**：
   - 输出与原图分辨率一致的分割图，每个像素点对应一个类别标签。

为了更高效地利用特征，RefineNet 在较深层次添加了更多瓶颈块（Bottleneck Blocks），以在空间分辨率较低的层级上实现性能和效率的平衡。

## RTMO1

我们的模型采用类似 YOLO 的架构（如图2所示）。主干网络使用 **CSPDarknet**【10】，并通过一个混合编码器【29】处理最后三层特征图

## RTMO2

==现有的坐标分类方法存在一个显著问题，即它们采用静态的区间分配策略==。

为了解决这一问题，我们提出了**动态坐标分类器（DCC）**，通过在两个 1D 热图中动态分配区间范围并生成区间表示，解决了==坐标分类在密集预测场景中的不兼容性==。

#### 动态区间分配

顶部-底部姿态估计器中的坐标分类技术通常在整个输入图像上分配区间【16, 23】，但==在单阶段方法中，由于每个目标仅占据图像的一小部分，这种策略会导致大量区间的浪费==。DFL【21】尝试在锚点附近的预定义范围内分配区间，但可能会漏掉大实例的关键点，并对小实例产生显著的量化误差。

==DCC 通过动态分配区间以对齐每个实例的边界框，从而确保局部化覆盖==。边界框最初通过点卷积层回归生成，并扩展 1.25 倍以覆盖所有关键点，即使在预测不准确的情况下。这些扩展后的边界框在水平和垂直方向上均匀划分为 $B_x$ 和 $B_y$ 个区间。水平轴上每个区间的 $x$ 坐标计算如下：
$$
x_i = x_l + \frac{(x_r - x_l) \cdot (i - 1)}{B_x - 1}
$$


其中，$x_r$ 和 $x_l$ 分别表示边界框的右边界和左边界，i 为区间索引，从 1 到 $B_x$。垂直方向的计算方法与此类似。

#### 动态区间编码

在 DCC 中，每个网格的区间位置因其预测的边界框不同而变化，这与以前的固定区间坐标方法【16, 23】不同。传统方法为所有网格共享区间表示，而==DCC 在每次推理时动态生成区间表示==。我们通过将每个区间的坐标编码为位置编码（Positional Encoding）来生成区间特定的表示。具体来说，位置编码采用如下公式：
$$
[\text{PE}(x_i)]_c = \begin{cases}  \sin\left(\frac{x_i}{t^{c/C}}\right), & \text{当 } c \text{ 是偶数时} \\ \cos\left(\frac{x_i}{t^{(c-1)/C}}\right), & \text{当 } c \text{ 是奇数时}  \end{cases}
$$


其中，$t$ 是温度参数，$c$ 是索引，$C$ 是总维度数。我们通过一个可学习的线性变换优化位置编码的适应性。

动态坐标分类器的主要目标是==根据区间坐标和关键点特征准确预测每个区间中关键点出现的概率==。关键点特征由姿态特征中提取，并通过门控注意单元（Gated Attention Unit, GAU）模块进一步优化，以增强关键点之间的一致性。概率热图通过关键点特征与区间位置编码的相似性计算生成，公式如下：
$$
\hat{p}_k(x_i) = \frac{e^{f_k \cdot \phi(\text{PE}(x_i))}}{\sum_{j=1}^{B_x} e^{f_k \cdot \phi(\text{PE}(x_j))}}
$$


其中，$f_k$ 表示第 $k$ 个关键点的特征向量。

## RTMO3

基于最大似然估计的坐标分类

在分类任务中，通常使用 one-hot 目标和交叉熵损失进行优化。为了提高性能，一些方法采用标签平滑技术，例如 SimCC【23】和 RTMPose【16】中的高斯标签平滑，与 KLD 结合使用。高斯分布的均值 $\mu_x, \mu_y$ 和方差 $\sigma^2$ 被设为注释坐标和预定义参数，其目标分布为：
$$
p_k(x_i | \mu_x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x_i - \mu_x)^2}{2\sigma^2}}
$$

重要的是，我们注意到 $p_k(x_i \mid \mu_x)$ 数学上等价于在高斯误差模型下，真实值 $x_i$ 的似然估计 $p_k(\mu_x \mid x_i)$。

这种对称性源于高斯分布关于其均值对称的特性。因此可以用负对数似然损失来优化：
$$
L_{mle}^{(x)} = -\log \left[ \sum_{i=1}^{B_x} \frac{1}{\sigma} e^{-\frac{|x_i - \mu_x|}{2\sigma s}} \hat{p}_k(x_i) \right]
$$


其中，$s$ 是实例大小的归一化参数，$\sigma$ 是可预测的方差。最大化该似然值能够建模标注数据的真实分布。总损失为：
$$
L_{mle} = L_{mle}^{(x)} + L_{mle}^{(y)}
$$


与 KLD 不同，MLE 损失允许学习方差以表示不确定性，从而自适应调整困难样本和简单样本的优化。