# 组会讲稿

## CPN1

我们用 $Y_p \in Z \subseteq \mathbb{R}^2$ 表示 $P$ 个解剖学标志点（即部位）的像素位置，其中 $Z$ 是图像中的所有 $(u, v)$ 位置的集合。我们的目标是预测所有 $P$ 个部位的位置 $Y = (Y_1, \dots, Y_P)$。

一个姿态机器【29】（见图 2(a) 和 2(b)）由一系列==多类别预测器 $g_t(\cdot)$== 组成，这些预测器被训练用于在层级结构的每一级预测每个部位的位置。在每个阶段 $t \in {1, \dots, T}$ 中，==分类器 $g_t$==用于预测每个部位的信念值，从而为每个部位 $Y_p = z, \forall z \in Z$ 生成belief map：
$$
g_1(x_z) \to \{b_{p1}(Y_p = z)\}_{p \in \{0, \dots, P\}},
$$
其中，==$b_{p1}(Y_p = z)$ 是分类器 $g_1$ 在图像位置 $z$ 为第 $p$ 个部位预测的得分==。我们将所有部位 $p$ 在图像每个位置 $z = (u, v)^T$ 处的信念表示为 $b_{p1} \in \mathbb{R}^{w \times h}$，其中 $w$ 和 $h$ 分别是图像的宽度和高度，即：
$$
b_{pt}[u, v] = b_{pt}(Y_p = z)。
$$
为简化表示，我们将所有部位的belief map集合表示为 $b_t \in \mathbb{R}^{w \times h \times (P+1)}$（==$P$ 个部位加一个背景==）。

在后续阶段中，分类器预测每个部位的信念 $Y_p = z, \forall z \in Z$，此时的输入包括：(1) 图像数据 $x_t^z \in \mathbb{R}^d$ 的特征，以及 (2) 来自上一阶段的分类器的belief map的上下文信息：
$$
g_t(x'_z, \psi_t(z, b_{t-1})) \to \{b_{pt}(Y_p = z)\}_{p \in \{0, \dots, P+1\}}，
$$
其中，$\psi_{t>1}(\cdot)$ 是从 $b_{t-1}$ 到上下文特征的映射。在每个阶段中，计算的belief map会逐步优化每个部位的位置估计。需要注意的是，==后续阶段的图像特征 $x'_z$ 允许不同于第一阶段的图像特征 $x$==。文献【29】中提出的姿态机器使用提升随机森林作为预测器（$g_t$），所有阶段中使用固定的手工设计图像特征（$x' = x$），以及固定的手工设计==上下文特征映射（$\psi_t(\cdot)$）==来捕捉空间上下文。

## CPN2

==第一阶段的belief map是由一个小感受野的网络从局部图像信息中生成的。在第二阶段中，我们设计了一个网络，通过极大地增加有效感受野以捕获长距离的空间交互==。==大感受野==可以通过以下方式实现：

1. **池化操作**，但可能会以精度为代价；
2. **增大卷积核大小**，但会增加模型的参数数量；
3. **增加卷积层的数量**，但可能会导致训练过程中出现梯度消失问题。

我们在图 2(d) 中展示了第二阶段及后续阶段（$t \geq 2$）的网络设计和对应的感受野。在belief map上，我们==使用多层卷积来实现大的感受野，而不是依赖步幅较大的池化操作==。这种设计能够==以较少的模型参数实现大的感受野，同时保持较高的精度==。实验表明，即使在高精度范围内，步幅为 8 的网络与步幅为 4 的网络表现相当，但步幅为 8 的设计更容易实现较大的感受野。

## CPN3

我们通过在网络的每个阶段输出处定义一个损失函数来鼓励网络不断生成这样的表示，==该损失函数最小化预测belief map与理想belief map之间的 $l_2$ 距离==。==对于某个部位 $p$ 的理想belief map记为 $b_p^*(Y_p = z)$，其通过在每个身体部位 $p$ 的真实位置放置高斯峰值生成==。我们在每个阶段 $t$ 的输出处希望最小化的代价函数为：
$$
f_t = \sum_{p=1}^{P+1} \sum_{z \in Z} \| b_p^t(z) - b_p^*(z) \|_2^2。
$$
==完整架构的总体目标是将每个阶段的损失相加==，其表示为：
$$
F = \sum_{t=1}^T f_t。
$$
我们使用标准的随机梯度下降（Stochastic Gradient Descent, SGD）对网络的所有 $T$ 个阶段进行联合训练。为了在所有后续阶段共享图像特征 $x'$，我们在 $t \geq 2$ 的阶段中共享对应卷积层的权重（见图2）。

## Hourglass

每个Hourglass module的结构如下图所示，其中每个box都是一个残差结构。可以看到在top-down阶段，对于两个**相邻**分辨率的feature map，我们通过upsampling（这里用的是nearest neighbor upsampling）对较低分辨率的feature map进行上采样，然后通过**skip connection**将bottom-up阶段较高分辨率的feature map拿过来，此时再通过element-wise addition将这两部分特征进行合并。通过这种方式，在top-down阶段将不同分辨率下的特征逐步进行了融合。

## CPN1

我们的网络架构包括两个阶段：GlobalNet 和 RefineNet。==GlobalNet基于特征金字塔网络[24]学习良好的特征表示==。更重要的是，金字塔特征表示能够提供足够的上下文信息，这是推理遮挡和不可见关节所必需的。基于金字塔特征，==RefineNet通过在线难关键点挖掘损失显式地处理这些“难”关键点==。

## CPN2

### **GlobalNet 的结构**

1. **基于 ResNet 的网络架构**：
   - **GlobalNet** 通常基于成熟的卷积神经网络架构，如 **ResNet**，作为骨干网络。ResNet 的不同卷积层会生成不同尺度的特征图，例如 C2,C3,C4,C5C_2, C_3, C_4, C_5C2,C3,C4,C5 层，每一层的特征图提供了不同层次的空间分辨率和语义信息。
   - 浅层特征（如 C2C_2C2 和 C3C_3C3）包含较多的细节信息，但语义信息较少；而深层特征（如 C4C_4C4 和 C5C_5C5）包含丰富的语义信息，但空间分辨率较低。
2. **特征金字塔结构**：
   - 在 **GlobalNet** 中，采用了一种类似于 **特征金字塔网络（FPN）** 的结构，逐步提取不同尺度的特征。通过上采样和下采样操作，**GlobalNet** 融合了不同层次的特征图，增强了对图像中目标的全局上下文理解。
   - 在 **GlobalNet** 中，上采样和下采样层的融合有助于同时保持较高的空间分辨率和丰富的语义信息。
3. **多尺度特征融合**：
   - **GlobalNet** 通过融合不同尺度的特征图来处理多尺度目标，确保网络能够同时识别大目标和小目标，尤其是在复杂场景中的小关节点（如手指、脚趾等）的定位。
   - 在网络的不同层次，浅层特征负责提供细节信息，深层特征负责提供全局语义信息。通过融合这些信息，GlobalNet 能够提高关节定位的准确性。
4. **特征整合与逐元素相加**：
   - 在多尺度特征融合过程中，**GlobalNet** 在不同特征图之间进行逐元素相加（element-wise addition），确保每个层的特征信息都能够有效传递到后续层。
   - 为了确保通道数匹配，通常使用 **$1 \times 1$ 卷积** 来调整不同层特征图的通道数，从而使得它们能够顺利进行相加操作。

## CPN3

### **RefineNet 的工作流程**

假设输入图像的大小为 $512 \times 512$：

1. **特征提取**：
   - 使用 ResNet 提取不同层次的特征（例如 $C2$, $C3$, $C4$, $C5$）。
   - 特征分辨率从高到低，语义信息从弱到强。
2. **特征细化**：
   - 将深层特征（低分辨率，强语义）通过上采样恢复到更高分辨率。
   - 与对应浅层特征进行融合，结合语义和细节信息。
3. **逐步细化**：
   - 多层 RefineNet 模块串联，逐层细化特征。
   - 最终生成高分辨率的分割预测。
4. **分割结果**：
   - 输出与原图分辨率一致的分割图，每个像素点对应一个类别标签。

为了更高效地利用特征，RefineNet 在较深层次添加了更多瓶颈块（Bottleneck Blocks），以在空间分辨率较低的层级上实现性能和效率的平衡。

## RTMO

**现有方法的挑战**：

- **Top-down 方法**：效率低，计算量大，难以处理多人场景。
- **One-stage 方法**：精度不足，实时性差。

#### **4. RTMO 的核心创新**

- **动态坐标分类器（DCC）**：
  - 动态调整区间分配，以适应不同大小的目标。
  - 增强空间信息的理解。
- **动态区间编码（Dynamic Bin Encoding, DBE）**：
  - 使用正弦位置编码（SPE）将每个区间的位置转化为特征向量。
  - 提升位置的精确表示。
- **门控注意单元（Gated Attention Unit, GAU）**：
  - 增强关键点特征之间的关系。
  - 提高定位精度，尤其是在复杂或密集场景中。

#### **5. 关键技术详解**

- **YOLO-like 网络架构**：
  - 主干网络：CSPDarknet
  - 颈部：多尺度特征融合
  - 网络头部：边界框、关键点坐标和可见性预测
- **动态坐标分类器**：
  - 动态分配区域，优化关键点的预测。
  - 使用高斯分布建模位置的不确定性。
- **最大似然估计（MLE）**：
  - 为坐标分类设计的损失函数，结合了不确定性建模和方差学习。
  - 通过学习方差调整难易样本的优化。