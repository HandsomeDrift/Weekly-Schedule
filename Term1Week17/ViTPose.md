# ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation

## 摘要

尽管在设计上没有考虑具体的领域知识，但简单的视觉Transformer在视觉识别任务中表现出了卓越的性能。然而，很少有人尝试探索这种简单结构在姿态估计任务中的潜力。在本文中，我们通过一个名为ViTPose的简单基线模型，从模型结构的简洁性、模型规模的扩展性、训练范式的灵活性以及模型之间知识的可迁移性等方面，展示了纯视觉Transformer在姿态估计任务中的惊人能力。具体来说，ViTPose使用简单且非分层的视觉Transformer作为骨干网络，提取给定人体实例的特征，并利用轻量级解码器进行姿态估计。通过利用Transformer可扩展的模型容量和高并行性，模型规模可以从100M参数扩展到1B参数，设置了吞吐量和性能之间新的帕累托前沿。此外，ViTPose在注意力类型、输入分辨率、预训练和微调策略以及多种姿态任务处理方面表现出极大的灵活性。我们还通过一个简单的知识Token实验证明，大规模ViTPose模型的知识可以轻松转移到小规模模型中。实验结果显示，我们的基本ViTPose模型在具有挑战性的MS COCO关键点检测基准上优于代表性方法，而最大的模型在MS COCO test-dev集上达到了新的最先进水平（80.9 AP）。代码和模型已公开：https://github.com/ViTAE-Transformer/ViTPose。

## 引言

人体姿态估计是计算机视觉中的一项基础任务，拥有广泛的现实世界应用 [51, 29]。其目标是定位人体的解剖学关键点，由于遮挡、裁剪、尺度变化以及人体外观的多样性，这一任务具有较高的挑战性。为了解决这些问题，基于深度学习的方法取得了迅速发展 [37, 42, 36, 50]，这些方法通常使用卷积神经网络（CNN）来处理这一具有挑战性的任务。

近年来，视觉Transformer [13, 31, 10, 34, 32] 在许多视觉任务中展现出了巨大的潜力。受其成功的启发，不同的视觉Transformer结构被应用于姿态估计任务。这些方法中的大多数采用CNN作为骨干网络，然后使用设计精巧的Transformer结构来提取特征并建模人体关键点之间的关系。例如，PRTR [23] 结合了Transformer编码器和解码器，以级联的方式逐步优化预测关键点的位置。TokenPose [27] 和 TransPose [44] 则采用了仅由编码器组成的Transformer结构，处理由CNN提取的特征。而另一方面，HRFormer [48] 使用Transformer直接提取特征，并通过多分辨率并行Transformer模块引入高分辨率表示。这些方法在姿态估计任务上取得了优异的性能。然而，它们要么需要额外的CNN来提取特征，要么需要精心设计Transformer结构以适应任务需求。这促使我们从相反的方向思考：纯视觉Transformer在姿态估计任务中表现究竟如何？

为了回答这一问题，我们提出了一个名为 **ViTPose** 的简单基线模型，并在 MS COCO Keypoint 数据集 [28] 上展示了其潜力。具体来说，ViTPose 使用简单且非分层的视觉Transformer [13] 作为骨干网络，从给定的人体实例中提取特征图，这些骨干网络通过遮挡图像建模的预训练任务（例如 MAE [15]）进行预训练，从而提供良好的初始化。随后，一个轻量级的解码器通过对特征图进行上采样并回归关键点对应的热图来处理这些提取的特征，该解码器由两层反卷积层和一层预测层组成。尽管该模型的结构没有复杂的设计，ViTPose 在具有挑战性的 MS COCO Keypoint 测试集上达到了80.9 AP的最新性能（SOTA）。需要注意的是，本文并不主张算法本身的优越性，而是提出了一个简单且稳健的Transformer基线，具有出色的姿态估计性能。

除了优异的性能，我们还从以下几个方面展示了ViTPose的惊人能力：简单性、可扩展性、灵活性以及可迁移性。

1. **简单性**：得益于视觉Transformer的强大特征表达能力，ViTPose框架非常简单。例如，其骨干编码器的设计不需要任何特定领域知识，仅通过简单堆叠多个Transformer层即可实现非分层编码器结构。解码器可以进一步简化为一个上采样层和一个卷积预测层，且性能下降可忽略不计。这样的结构简单性使得ViTPose在推理速度和性能之间达到了新的帕累托前沿（如图1所示）。
2. **可扩展性**：这种简单性使得ViTPose展现了卓越的扩展能力。因此，它受益于可扩展预训练视觉Transformer的快速发展。具体而言，可以通过堆叠不同数量的Transformer层或调整特征维度（例如，使用ViT-B、ViT-L或ViT-H）来控制模型规模，以平衡推理速度和性能，从而满足不同的部署需求。
3. **灵活性**：我们展示了ViTPose在训练范式上的高度灵活性。ViTPose可以通过微小的修改适应不同的输入分辨率和特征分辨率，且输入分辨率越高，姿态估计结果越精确。此外，除了常规的单一姿态数据集训练方法，我们可以通过添加额外的解码器，灵活地将其调整为多姿态数据集训练方式，形成联合训练流程，并显著提升性能。由于解码器非常轻量级，这种训练范式只带来极少的额外计算开销。此外，当使用较小的无标签数据集进行预训练或冻结注意力模块进行微调时，ViTPose仍能获得SOTA性能，相较于完全微调的训练范式，训练成本更低。
4. **可迁移性**：通过添加一个额外的可学习知识Token，小规模的ViTPose模型的性能可以轻松通过大规模ViTPose模型的知识转移得到提升，展示了ViTPose的良好迁移能力。

总之，本文的贡献包括以下三点：

1. 我们提出了一个名为ViTPose的简单有效基线模型用于人体姿态估计。尽管没有复杂的结构设计或框架，该模型在 MS COCO Keypoint 数据集上达到了SOTA性能。
2. ViTPose展现了惊人的能力，包括结构简单性、模型规模可扩展性、训练范式灵活性以及知识可迁移性。这些能力为基于视觉Transformer的姿态估计任务建立了一个强有力的基线，并可能为该领域的进一步发展提供启发。
3. 在流行基准数据集上进行了全面的实验，以研究和分析ViTPose的能力。使用超大规模视觉Transformer模型（即ViTAE-G [52]）作为骨干网络的单一ViTPose模型，在 MS COCO Keypoint 测试集上达到了最佳80.9 AP的性能。