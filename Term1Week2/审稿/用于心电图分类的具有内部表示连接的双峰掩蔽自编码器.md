# 用于心电图分类的具有内部表示连接的双峰掩蔽自编码器

## 摘要

时间序列的自监督方法被广泛应用，心电图（ECG）分类任务也从中受益。一个主流的范式是掩码数据建模（masked data modeling），该方法利用可见的数据部分来重建被掩码的部分，从而帮助获得对下游任务有用的表示。然而，传统方法主要关注时间域信息，并对编码器在重建方面提出过高要求，进而削弱了模型的判别能力。本文提出了一种用于心电图分类的**双模态掩码自动编码器与内部表示连接**（BMIRC）。一方面，BMIRC在掩码预训练过程中融合了ECG的频谱信息，增强了模型对心电图的全面理解；另一方面，它从编码器到解码器建立了**内部表示连接**（IRC），为解码器提供了多层次的信息以帮助重建，从而让编码器可以专注于建模判别性表示。我们在三个不同的ECG数据集上进行了全面实验，以验证BMIRC的有效性。实验结果表明，无论是在同域（在同一数据集上进行预训练和微调）还是跨域（在不同数据集上进行预训练和微调）设置中，BMIRC在大多数场景中均超越了竞争性基线方法。代码已公开，地址为：https://github.com/Envy-Clouds/BMIRC。

关键词：心电图；频谱；双模态；掩码自动编码器；内部表示连接

## 概述

作为一种非侵入性诊断程序，ECG是检测心律失常最方便和最有效的工具[1]。最初，ECG分析仅由人类专家执行，这种做法容易受到主观性和不同专业水平的影响，电脑科技的进步，大大提高了心电图的效率和准确性，基于辅助诊断方法，在医学界获得越来越多的关注[2，3]。


近年来，由于采用了深度学习方法，ECG分类任务取得了显着进步。研究人员已经通过利用卷积神经网络（CNN），变压器或其合并展示了令人满意的结果，表明此类方法在该领域的有效性[4，5]。


值得注意的是，这些方法主要在监督学习的框架内运行，需要大量的标记数据进行训练。此外，这种训练范式限制了未标记和外生数据源的利用。


在与深度学习相关的其他领域，如计算机视觉（CV）和自然语言处理（NLP），研究人员采用自监督学习来解决上述挑战[6，7]。这种范式利用辅助任务从未标记的数据中提取监督信息，使网络能够获得有利于后续任务的表示。一个用自我监督预处理增强的模型，training从未标记的数据中学习额外的知识，从而与仅通过监督训练训练的模型相比，在监督训练之后实现上级性能。值得注意的是，这些额外的知识可以应用于其他数据集以产生进一步的改进。

最近，用于时间序列分析的自监督方法已经变得流行，ECG分类任务也从这些进步中获益[8-14]。

掩蔽数据建模是该领域的主流范式。与依赖于预定一致性假设的对比学习不同，掩蔽数据建模关注数据的固有特征。这一特征增强了其适用性，从而促使我们将其应用于ECG分类任务。

然而，这些方法大多只关注时域信息，忽略了来自其他模态或视角的信息，考虑到多模态数据之间的互补性[15]，仅依赖于单一模态的方法无法捕捉到更全面的信息，从而限制了模型的推理和判断能力。

在掩模数据建模的框架内，当重建目标是原始数据时，最具代表性的范例是掩模自编码器（MAE）。它包括负责编码可见数据的编码器和负责重建掩模数据的解码器。在ECG分类任务中，许多心律类别与形态特征相关联。例如，心房颤动（AF）的典型特征是P波的缺失。2 MAE致力于挖掘原始数据的潜在特征，帮助模型学习这些有区别的细节。3这种动力驱使我们利用这种范式来推进我们的研究。

然而，由于自监督重建任务和监督下游任务之间存在差距，MAE的应用面临一些问题。根据信息瓶颈理论[16]，在典型的监督学习范式下，靠近输入的层捕获更多的低级信息，而靠近输出的层包含更多的高级信息。与上述范式不同，MAE编码器的浅层和MAE解码器的深层都包含丰富的低级信息。此外，各种级别的信息被保存在网络的中间层中，高级信息通常在编码器的深层中找到[17]。低级信息属于常见的形态细节，它与原始数据密切相关。高级信息属于判别表示，这对于下游分类任务至关重要。解码器依赖于编码器的最终输出将高级表示转换为低级表示。

然而，这样的解码过程迫使编码器过度集中于较低级别的信息，旨在帮助解码器更容易地完成重建。简而言之，这种配置导致编码器过度关注重建，从而限制了其学习高级区分表示的能力。

为了解决上述问题，我们提出了一种新的双峰掩蔽自编码器框架，表示为BMIRC。具体来说，我们采用离散傅立叶变换（DFT）将ECG转换为频谱，并将其视为一个独立的模态，这有助于补充模型用于学习的数据源。在频域分析已被广泛应用的先前研究中，这些方法通常涉及直接提取频域特征，随后将其输入到网络中[18]，或者采用双编码器在时域和频域中进行对比表示学习[9]我们利用频谱的方法涉及构建一个双峰联合编码器，旨在学习ECG和频谱的联合表示。进行屏蔽数据重构，ECG和频谱相互补充，促进可转移表示的学习。基于MAE范式，我们追求获得更多区分性表示涉及到各级信息的重用。具体来说，从编码器的中间层提取的表示被集成到解码器的各个层中，设计了一种门控表示混合器（GRM）来促进融合。我们将这个过程称为内部表示连接（IRC）。这种方法为解码器提供了各种级别的信息来帮助重建，减轻了解码器的负担，同时鼓励编码器获得更具鉴别力的表示。我们的主要贡献总结如下：

- 我们提出了一种新颖的用于时频联合建模的双模掩蔽自编码器框架。该方法将心电图的频谱集成到掩蔽预训练过程中，使双模联合编码器能够学习全面且通用的表示
- 我们在编码器和解码器之间建立内部表示连接（IRC），并设计了一个门控表示混合器（GRM）来复用不同层次的信息，从而减轻了解码器的重构负担，同时促进编码器获得更具鉴别力的表示。
- 通过在三个不同的ECG数据集上进行的综合实验，BMIRC在大多数情况下表现出优于竞争基线的上级性能，证明了所提出方法的有效性。

本文的后续部分结构如下：第2节介绍了多模态自监督学习和时间序列自监督学习的相关工作;第3节详细描述了所提出的BMIRC;第4节描述了实验设计;第5节评估了所提出方法的性能;第6节概括了本文的结论并描述了未来工作的方向。

